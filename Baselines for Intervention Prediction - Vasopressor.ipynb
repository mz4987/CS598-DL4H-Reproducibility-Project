{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/wei/Documents/UIUC/2022Spring/CS598DLH/Project/Project_Coding/CS598-DL4H-Reproducibility-Project/utils')\n",
    "from simple_impute import simple_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVENTION = 'vaso'\n",
    "RANDOM = 0\n",
    "MAX_LEN = 240\n",
    "SLICE_SIZE = 6\n",
    "GAP_TIME = 6\n",
    "PREDICTION_WINDOW = 4\n",
    "OUTCOME_TYPE = 'all'\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_KEY = {'ONSET': 0, 'CONTROL': 1, 'ON_INTERVENTION': 2, 'WEAN': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = 'data/all/all_hourly_data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 8.06 s, total: 19.6 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = pd.read_hdf(DATAFILE,'vitals_labs')\n",
    "Y = pd.read_hdf(DATAFILE,'interventions')\n",
    "static = pd.read_hdf(DATAFILE,'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[[INTERVENTION]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X :  (2200954, 312)\n",
      "Shape of Y :  (2200954, 1)\n",
      "Shape of static :  (34472, 28)\n"
     ]
    }
   ],
   "source": [
    "print ('Shape of X : ', X.shape)\n",
    "print ('Shape of Y : ', Y.shape)\n",
    "print ('Shape of static : ', static.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split, Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 ms, sys: 8.08 ms, total: 43.1 ms\n",
      "Wall time: 46.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_ids, test_ids = train_test_split(static.reset_index(), test_size=0.2, \n",
    "                                       random_state=RANDOM, stratify=static['mort_hosp'])\n",
    "split_train_ids, val_ids = train_test_split(train_ids, test_size=0.125, \n",
    "                                            random_state=RANDOM, stratify=train_ids['mort_hosp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and Standardization of Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1884: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, val, pi)\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 53s, sys: 5min 2s, total: 50min 56s\n",
      "Wall time: 51min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_clean = simple_imputer(X,train_ids['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(x):# normalize\n",
    "    mins = x.min()\n",
    "    maxes = x.max()\n",
    "    x_std = (x - mins) / (maxes - mins)\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_time_since_measurement(x):\n",
    "    idx = pd.IndexSlice\n",
    "    x = np.where(x==100, 0, x)\n",
    "    means = x.mean()\n",
    "    stds = x.std()\n",
    "    x_std = (x - means)/stds\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 48s, sys: 42.3 s, total: 5min 31s\n",
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "X_std = X_clean.copy()\n",
    "X_std.loc[:,idx[:,'mean']] = X_std.loc[:,idx[:,'mean']].apply(lambda x: minmax(x))\n",
    "X_std.loc[:,idx[:,'time_since_measured']] = X_std.loc[:,idx[:,'time_since_measured']].apply(lambda x: std_time_since_measurement(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.columns = X_std.columns.droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization of Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(age):\n",
    "    if age > 10 and age <= 30: \n",
    "        cat = 1\n",
    "    elif age > 30 and age <= 50:\n",
    "        cat = 2\n",
    "    elif age > 50 and age <= 70:\n",
    "        cat = 3\n",
    "    else: \n",
    "        cat = 4\n",
    "    return cat\n",
    "\n",
    "def categorize_ethnicity(ethnicity):\n",
    "    if 'AMERICAN INDIAN' in ethnicity:\n",
    "        ethnicity = 'AMERICAN INDIAN'\n",
    "    elif 'ASIAN' in ethnicity:\n",
    "        ethnicity = 'ASIAN'\n",
    "    elif 'WHITE' in ethnicity:\n",
    "        ethnicity = 'WHITE'\n",
    "    elif 'HISPANIC' in ethnicity:\n",
    "        ethnicity = 'HISPANIC/LATINO'\n",
    "    elif 'BLACK' in ethnicity:\n",
    "        ethnicity = 'BLACK'\n",
    "    else: \n",
    "        ethnicity = 'OTHER'\n",
    "    return ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 29.5 ms, total: 180 ms\n",
      "Wall time: 206 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# use gender, first_careunit, age and ethnicity for prediction\n",
    "static_to_keep = static[['gender', 'age', 'ethnicity', 'first_careunit', 'intime']]\n",
    "static_to_keep.loc[:, 'intime'] = static_to_keep['intime'].astype('datetime64').apply(lambda x : x.hour)\n",
    "static_to_keep.loc[:, 'age'] = static_to_keep['age'].apply(categorize_age)\n",
    "static_to_keep.loc[:, 'ethnicity'] = static_to_keep['ethnicity'].apply(categorize_ethnicity)\n",
    "static_to_keep = pd.get_dummies(static_to_keep, columns = ['gender', 'age', 'ethnicity', 'first_careunit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.68 s, sys: 18.1 s, total: 27.7 s\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# merge time series and static data\n",
    "X_merge = pd.merge(X_std.reset_index(), static_to_keep.reset_index(), on=['subject_id','icustay_id','hadm_id'])\n",
    "# add absolute time feature\n",
    "abs_time = (X_merge['intime'] + X_merge['hours_in'])%24\n",
    "X_merge.insert(4, 'absolute_time', abs_time)\n",
    "X_merge.drop('intime', axis=1, inplace=True)\n",
    "X_merge = X_merge.set_index(['subject_id','icustay_id','hadm_id','hours_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_std, X_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_matrix(x):\n",
    "    zeros = np.zeros((MAX_LEN, x.shape[1]-4))\n",
    "    x = x.values\n",
    "    x = x[:(MAX_LEN), 4:]\n",
    "    zeros[0:x.shape[0], :] = x\n",
    "    return zeros\n",
    "\n",
    "def create_y_matrix(y):\n",
    "    zeros = np.zeros((MAX_LEN, y.shape[1]-4))\n",
    "    y = y.values\n",
    "    y = y[:,4:]\n",
    "    y = y[:MAX_LEN, :]\n",
    "    zeros[:y.shape[0], :] = y\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 36.4 s, total: 1min\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = np.array(list(X_merge.reset_index().groupby('subject_id').apply(create_x_matrix)))\n",
    "y = np.array(list(Y.reset_index().groupby('subject_id').apply(create_y_matrix)))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = np.array(list(X_merge.reset_index().groupby('subject_id').apply(lambda x: x.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pd.Series(X_merge.reset_index()['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor shape:  (34472, 240, 330)\n",
      "Y tensor shape:  (34472, 240)\n",
      "lengths shape:  (34472,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X tensor shape: \", x.shape)\n",
    "print(\"Y tensor shape: \", y.shape)\n",
    "print(\"lengths shape: \", lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.16 ms, sys: 12.4 ms, total: 19.5 ms\n",
      "Wall time: 34.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_indices = np.where(keys.isin(train_ids['subject_id']))[0]\n",
    "test_indices = np.where(keys.isin(test_ids['subject_id']))[0]\n",
    "train_static = train_ids\n",
    "split_train_indices = np.where(keys.isin(split_train_ids['subject_id']))[0]\n",
    "val_indices = np.where(keys.isin(val_ids['subject_id']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x[split_train_indices]\n",
    "Y_train = y[split_train_indices]\n",
    "X_test = x[test_indices]\n",
    "Y_test = y[test_indices]\n",
    "X_val = x[val_indices]\n",
    "Y_val = y[val_indices]\n",
    "lengths_train = lengths[split_train_indices]\n",
    "lengths_val = lengths[val_indices]\n",
    "lengths_test = lengths[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  24129\n",
      "Validation size:  3448\n",
      "Test size:  6895\n"
     ]
    }
   ],
   "source": [
    "print(\"Training size: \", X_train.shape[0])\n",
    "print(\"Validation size: \", X_val.shape[0])\n",
    "print(\"Test size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3d_tensor_slices(X_tensor, Y_tensor, lengths):\n",
    "\n",
    "    num_patients = X_tensor.shape[0]\n",
    "    timesteps = X_tensor.shape[1]\n",
    "    num_features = X_tensor.shape[2]\n",
    "    X_tensor_new = np.zeros((lengths.sum(), SLICE_SIZE, num_features + 1))\n",
    "    Y_tensor_new = np.zeros((lengths.sum()))\n",
    "\n",
    "    current_row = 0\n",
    "    \n",
    "    for patient_index in range(num_patients):\n",
    "        x_patient = X_tensor[patient_index]\n",
    "        y_patient = Y_tensor[patient_index]\n",
    "        length = lengths[patient_index]\n",
    "\n",
    "        for timestep in range(length - PREDICTION_WINDOW - GAP_TIME - SLICE_SIZE):\n",
    "            x_window = x_patient[timestep:timestep+SLICE_SIZE]\n",
    "            y_window = y_patient[timestep:timestep+SLICE_SIZE]\n",
    "            x_window = np.concatenate((x_window, np.expand_dims(y_window,1)), axis=1)\n",
    "            result_window = y_patient[timestep+SLICE_SIZE+GAP_TIME:timestep+SLICE_SIZE+GAP_TIME+PREDICTION_WINDOW]\n",
    "            result_window_diff = set(np.diff(result_window))\n",
    "            #if 1 in result_window_diff: pdb.set_trace()\n",
    "            gap_window = y_patient[timestep+SLICE_SIZE:timestep+SLICE_SIZE+GAP_TIME]\n",
    "            gap_window_diff = set(np.diff(gap_window))\n",
    "\n",
    "            #print result_window, result_window_diff\n",
    "\n",
    "            if OUTCOME_TYPE == 'binary':\n",
    "                if max(gap_window) == 1:\n",
    "                    result = None\n",
    "                elif max(result_window) == 1:\n",
    "                    result = 1\n",
    "                elif max(result_window) == 0:\n",
    "                    result = 0\n",
    "                if result != None:\n",
    "                    X_tensor_new[current_row] = x_window\n",
    "                    Y_tensor_new[current_row] = result\n",
    "                    current_row += 1\n",
    "\n",
    "            else: \n",
    "                if 1 in gap_window_diff or -1 in gap_window_diff:\n",
    "                    result = None\n",
    "                elif (len(result_window_diff) == 1) and (0 in result_window_diff) and (max(result_window) == 0):\n",
    "                    result = CHUNK_KEY['CONTROL']\n",
    "                elif (len(result_window_diff) == 1) and (0 in result_window_diff) and (max(result_window) == 1):\n",
    "                    result = CHUNK_KEY['ON_INTERVENTION']\n",
    "                elif 1 in result_window_diff: \n",
    "                    result = CHUNK_KEY['ONSET']\n",
    "                elif -1 in result_window_diff:\n",
    "                    result = CHUNK_KEY['WEAN']\n",
    "                else:\n",
    "                    result = None\n",
    "\n",
    "                if result != None:\n",
    "                    X_tensor_new[current_row] = x_window\n",
    "                    Y_tensor_new[current_row] = result\n",
    "                    current_row += 1\n",
    "\n",
    "    X_tensor_new = X_tensor_new[:current_row,:,:]\n",
    "    Y_tensor_new = Y_tensor_new[:current_row]\n",
    "\n",
    "    return X_tensor_new, Y_tensor_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.3 s, sys: 20.3 s, total: 1min 12s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train, y_train = make_3d_tensor_slices(X_train, Y_train, lengths_train)\n",
    "x_val, y_val = make_3d_tensor_slices(X_val, Y_val, lengths_val)\n",
    "x_test, y_test = make_3d_tensor_slices(X_test, Y_test, lengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_classes = label_binarize(y_train, classes=range(NUM_CLASSES))\n",
    "y_val_classes = label_binarize(y_val, classes=range(NUM_CLASSES))\n",
    "y_test_classes = label_binarize(y_test, classes=range(NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, Y_train, X_test, Y_test, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:  (1108165, 6, 331)\n",
      "shape of x_val:  (160938, 6, 331)\n",
      "shape of x_test:  (314458, 6, 331)\n"
     ]
    }
   ],
   "source": [
    "print('shape of x_train: ', x_train.shape)\n",
    "print('shape of x_val: ', x_val.shape)\n",
    "print('shape of x_test: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_col = 17 #static_to_keep.shape[1] - 1\n",
    "time_series_col = 124 #X_merge.shape[1] - static_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_static(x):\n",
    "    x_static = x[:,0,time_series_col:x.shape[2]-1]\n",
    "    x_timeseries = np.reshape(x[:,:,:time_series_col],(x.shape[0], -1))\n",
    "    x_int = x[:,:,-1]\n",
    "    x_concat = np.concatenate((x_static, x_timeseries, x_int), axis=1)\n",
    "    return x_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate hourly features\n",
    "x_train_concat = remove_duplicate_static(x_train)\n",
    "x_val_concat = remove_duplicate_static(x_val)\n",
    "x_test_concat = remove_duplicate_static(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1108165, 956)\n",
      "(160938, 956)\n",
      "(314458, 956)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_concat.shape)\n",
    "print(x_val_concat.shape)\n",
    "print(x_test_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "np.random.seed(RANDOM)\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l2']),\n",
    "    'solver': Choice(['sag']),\n",
    "    'max_iter': Choice([100, 200]),\n",
    "    'class_weight': Choice(['balanced']),\n",
    "    'multi_class': Choice(['multinomial']),\n",
    "    'random_state': Choice([RANDOM])\n",
    "})\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "        \n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 200),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "    'class_weight': Choice(['balanced']),\n",
    "    'random_state': Choice([RANDOM])\n",
    "})\n",
    "RF_hyperparams_list = RF_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_train, X_val, X_test):\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        M = model(**hyperparams)\n",
    "        M.fit(X_train, y_train)\n",
    "        s = roc_auc_score(y_val_classes, M.predict_proba(X_val),average='macro')\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_train, X_val, X_test)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_train, X_val, X_test):\n",
    "    best_M = model(**best_hyperparams)\n",
    "    best_M.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    y_pred  = best_M.predict_proba(X_test)\n",
    "    idx = np.argmax(y_pred, axis=-1)\n",
    "    y_pred_label = np.zeros(y_pred.shape)\n",
    "    y_pred_label[np.arange(y_pred_label.shape[0]), idx] = 1\n",
    "    auc   = roc_auc_score(y_test_classes, y_pred, average=None)\n",
    "    aucmacro = roc_auc_score(y_test_classes, y_pred, average='macro')\n",
    "    accuracy =  accuracy_score(y_test_classes, y_pred_label)\n",
    "    f1 = f1_score(y_test_classes, y_pred_label, average='macro')\n",
    "    auprc = average_precision_score(y_test_classes, y_pred_label, average='macro')\n",
    "    return auc, aucmacro, accuracy, f1, auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results for model RF, (AUC, Macro_AUC, Accuracy, F1 Macro, AUPRC Macro)\n",
      "(array([0.69881568, 0.98137489, 0.98373734, 0.93980826]), 0.9009340424482322, 0.8394316570098391, 0.4830239903218068, 0.4132143300957052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results for model LR, (AUC, Macro_AUC, Accuracy, F1 Macro, AUPRC Macro)\n",
      "(array([0.68609284, 0.98189146, 0.98415233, 0.93891644]), 0.8977632681603341, 0.7652691297406967, 0.4576141034151877, 0.40304304425725024)\n",
      "CPU times: user 9h 34min, sys: 9min 45s, total: 9h 43min 45s\n",
      "Wall time: 9h 51min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = {}\n",
    "for model_name, model, hyperparams_list in [('RF', RandomForestClassifier, RF_hyperparams_list), \n",
    "                                            ('LR', LogisticRegression, LR_hyperparams_list)]:\n",
    "    if model_name not in results: results[model_name] = {}\n",
    "\n",
    "    results[model_name] = run_basic(\n",
    "        model, hyperparams_list, x_train_concat, x_val_concat, x_test_concat)\n",
    "    print(\"Final results for model %s, (AUC, Macro_AUC, Accuracy, F1 Macro, AUPRC Macro)\" % (model_name))\n",
    "    print(results[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow import set_random_seed\n",
    "# 04232022\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "import keras\n",
    "#from keras.models import Sequential, load_model\n",
    "# 04232022\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, RepeatVector, Lambda\n",
    "from keras.layers import Input, Conv2D, Conv1D, Conv3D, MaxPooling2D, MaxPooling1D\n",
    "from keras.layers import Concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1. 2. 3.], y=[2. 2. 2. ... 1. 1. 1.] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "#class_weight = [1,1,1,1]\n",
    "class_weight = dict(zip(range(len(class_weight)), class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 11:47:28.836489: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "8658/8658 [==============================] - 137s 16ms/step - loss: 0.9688 - accuracy: 0.7101 - val_loss: 0.6692 - val_accuracy: 0.7533\n",
      "Epoch 2/12\n",
      "8658/8658 [==============================] - 108s 12ms/step - loss: 0.8897 - accuracy: 0.7097 - val_loss: 0.4834 - val_accuracy: 0.8045\n",
      "Epoch 3/12\n",
      "8658/8658 [==============================] - 94s 11ms/step - loss: 0.8714 - accuracy: 0.7229 - val_loss: 0.7234 - val_accuracy: 0.6133\n",
      "Epoch 4/12\n",
      "8658/8658 [==============================] - 89s 10ms/step - loss: 0.8580 - accuracy: 0.7178 - val_loss: 0.7635 - val_accuracy: 0.5642\n",
      "CPU times: user 29min 33s, sys: 25min 41s, total: 55min 14s\n",
      "Wall time: 10min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e76ddac10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#sess = tf.Session(graph=tf.get_default_graph())\n",
    "# 04232022\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph())\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "np.random.seed(RANDOM)\n",
    "#set_random_seed(RANDOM)\n",
    "# 04232022\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "rn.seed(RANDOM)\n",
    "\n",
    "input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "inputs = Input(shape=input_shape)\n",
    "model = Conv1D(64, kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv2')(inputs)\n",
    "\n",
    "model = (MaxPooling1D(pool_size=3, strides=1))(model)\n",
    "\n",
    "model2 = Conv1D(64, kernel_size=4,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv3')(inputs)\n",
    "\n",
    "model2 = MaxPooling1D(pool_size=3, strides=1)(model2)\n",
    "\n",
    "model3 = Conv1D(64, kernel_size=5,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv4')(inputs)\n",
    "\n",
    "model3 = MaxPooling1D(pool_size=3, strides=1)(model3)\n",
    "\n",
    "models = [model, model2, model3]\n",
    "\n",
    "full_model = keras.layers.concatenate(models)\n",
    "full_model = Flatten()(full_model)\n",
    "full_model = Dense(128, activation='relu')(full_model)\n",
    "full_model = Dropout(DROPOUT)(full_model)\n",
    "full_model = Dense(NUM_CLASSES, activation='softmax')(full_model)\n",
    "\n",
    "# full_model = keras.models.Model(input=inputs, outputs=full_model)\n",
    "# 04232022\n",
    "full_model = keras.models.Model(inputs=inputs, outputs=full_model)\n",
    "\n",
    "#full_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#              optimizer=keras.optimizers.Adam(lr=.0005),\n",
    "#              metrics=['accuracy'])\n",
    "# 04232022\n",
    "full_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.optimizers.Adam(lr=.0005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "full_model.fit(x_train, y_train_classes,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          class_weight=class_weight,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:\n",
      "[0.64226107 0.9797334  0.98312769 0.93652878]\n",
      "AUC Macro:\n",
      "0.8854127354948824\n",
      "Accuracy: \n",
      "0.5626061350005406\n",
      "F1 Macro:\n",
      "0.4213467284313891\n",
      "AUPRC Macro: \n",
      "0.40456245996662565\n",
      "CPU times: user 39 s, sys: 35.2 s, total: 1min 14s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_preds_cnn = full_model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "idx = np.argmax(test_preds_cnn, axis=-1)\n",
    "test_preds_cnn_label = np.zeros(test_preds_cnn.shape)\n",
    "test_preds_cnn_label[np.arange(test_preds_cnn_label.shape[0]), idx] = 1\n",
    "print(\"AUC:\")\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average=None))\n",
    "print(\"AUC Macro:\")\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average='macro'))\n",
    "print(\"Accuracy: \")\n",
    "print(accuracy_score(y_test_classes, test_preds_cnn_label))\n",
    "print(\"F1 Macro:\")\n",
    "print(f1_score(y_test_classes, test_preds_cnn_label, average='macro'))\n",
    "print(\"AUPRC Macro: \")\n",
    "print(average_precision_score(y_test_classes, test_preds_cnn_label, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0. 1. 2. 3.], y=[2. 2. 2. ... 1. 1. 1.] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "#class_weight = [1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "KEEP_PROB = 0.8\n",
    "REGULARIZATION = 0.001\n",
    "NUM_HIDDEN = [512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceLabelling:\n",
    "\n",
    "    def __init__(self, data, target, dropout_prob, reg, num_hidden, class_weights):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.reg = reg\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = len(num_hidden)\n",
    "        self.num_classes = len(class_weights)\n",
    "        self.attn_length = 0\n",
    "        self.class_weights = class_weights\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def make_rnn_cell(self,\n",
    "                      attn_length=0,\n",
    "                      #base_cell=tf.nn.rnn_cell.BasicLSTMCell,\n",
    "                      # 04232022\n",
    "                      base_cell=tf.compat.v1.nn.rnn_cell.BasicLSTMCell,\n",
    "                      state_is_tuple=True):\n",
    "\n",
    "        attn_length = self.attn_length\n",
    "        input_dropout = self.dropout_prob\n",
    "        output_dropout = self.dropout_prob\n",
    "\n",
    "        cells = []\n",
    "        for num_units in self._num_hidden:\n",
    "            cell = base_cell(num_units, state_is_tuple=state_is_tuple)\n",
    "            #cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_dropout, output_keep_prob=output_dropout, seed=RANDOM)\n",
    "            # 04232022\n",
    "            cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_dropout, output_keep_prob=output_dropout, seed=RANDOM)\n",
    "            cells.append(cell)\n",
    "\n",
    "        #cell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=state_is_tuple)\n",
    "        # 04232022\n",
    "        cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=state_is_tuple)\n",
    "\n",
    "        return cell\n",
    "\n",
    "\n",
    "    # predictor for slices\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "\n",
    "        cell = self.make_rnn_cell\n",
    "\n",
    "        # Recurrent network.\n",
    "        #output, final_state = tf.nn.dynamic_rnn(cell,\n",
    "        # 04232022\n",
    "        output, final_state = tf.compat.v1.nn.dynamic_rnn(cell,\n",
    "            self.data,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        #with tf.variable_scope(\"model\") as scope:\n",
    "        # 04232022\n",
    "        with tf.compat.v1.variable_scope(\"model\") as scope:\n",
    "            \n",
    "            #tf.get_variable_scope().reuse_variables()\n",
    "            # 04232022\n",
    "            tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "\n",
    "            # final weights\n",
    "            num_classes = self.num_classes\n",
    "            weight, bias = self._weight_and_bias(self._num_hidden[-1], num_classes)\n",
    "    \n",
    "            # flatten + sigmoid\n",
    "            if self.attn_length > 0: \n",
    "                logits = tf.matmul(final_state[0][-1][-1], weight) + bias\n",
    "            else: \n",
    "                logits = tf.matmul(final_state[-1][-1], weight) + bias\n",
    "\n",
    "            prediction = tf.nn.softmax(logits)\n",
    "            \n",
    "            return logits, prediction\n",
    "\n",
    "        \n",
    "    @lazy_property\n",
    "    def cross_ent(self):\n",
    "        predictions = self.prediction[0]\n",
    "        real = tf.cast(tf.squeeze(self.target), tf.int32)\n",
    "        weights = tf.gather(self.class_weights, real)\n",
    "\n",
    "        #xent = tf.losses.sparse_softmax_cross_entropy(labels=real, logits=predictions, weights=weights)\n",
    "        # 04232022\n",
    "        xent = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=real, logits=predictions, weights=weights)\n",
    "        \n",
    "        loss = tf.reduce_mean(xent) #shape 1\n",
    "        ce = loss\n",
    "        #l2 = self.reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        # 04232022\n",
    "        l2 = self.reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\n",
    "        ce += l2\n",
    "        return ce\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.0003\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        # 04232022\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cross_ent)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        prediction = tf.argmax(self.prediction[1], 1)\n",
    "        real = tf.cast(self.target, tf.int32)\n",
    "        prediction = tf.cast(prediction, tf.int32)\n",
    "        mistakes = tf.not_equal(real, prediction)\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        #mistakes = tf.reduce_sum(mistakes, reduction_indices=0)\n",
    "        # 04232022\n",
    "        mistakes = tf.compat.v1.reduce_sum(mistakes, reduction_indices=0)\n",
    "        total = 128\n",
    "        #mistakes = tf.divide(mistakes, tf.to_float(total))\n",
    "        # 04232022\n",
    "        mistakes = tf.divide(mistakes, tf.compat.v1.to_float(total))\n",
    "        return mistakes\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        #weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        # 04232022\n",
    "        weight = tf.compat.v1.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def summaries(self):\n",
    "        tf.summary.scalar('loss', tf.reduce_mean(self.cross_ent))\n",
    "        tf.summary.scalar('error', self.error)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /var/folders/1f/xrh305k565d52tbr14203b9m0000gn/T/ipykernel_18276/2919458566.py:65: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/wei/opt/anaconda3/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/xrh305k565d52tbr14203b9m0000gn/T/ipykernel_18276/2919458566.py:43: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = base_cell(num_units, state_is_tuple=state_is_tuple)\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:754: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "/Users/wei/opt/anaconda3/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:757: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wei/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "AUC:\n",
      "[0.72113193 0.98019073 0.98291821 0.93965718]\n",
      "AUC Macro:\n",
      "0.9059745126834968\n",
      "Accuracy: \n",
      "0.8006252027297763\n",
      "F1 Macro:\n",
      "0.4633670158367899\n",
      "AUPRC Macro: \n",
      "0.4021146786069169\n",
      "CPU times: user 19h 9min 24s, sys: 10h 11min 37s, total: 1d 5h 21min 1s\n",
      "Wall time: 5h 22min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "# 04232022\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "#set_random_seed(RANDOM)\n",
    "# 04232022\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "#config = tf.ConfigProto(allow_soft_placement = True)\n",
    "# 04232022\n",
    "config = tf.compat.v1.ConfigProto(allow_soft_placement = True)\n",
    "# if attn_length > 0:\n",
    "#     # weights file initialized\n",
    "#     weight_file = 'weights.txt'\n",
    "#     with open(weight_file, 'a') as the_file:\n",
    "#         pass\n",
    "\n",
    "#with tf.Session(config = config) as sess, tf.device('/cpu:0'):\n",
    "# 04232022\n",
    "with tf.compat.v1.Session(config = config) as sess, tf.device('/cpu:0'):\n",
    "    _, length, num_features = x_train.shape\n",
    "    num_data_cols = num_features\n",
    "\n",
    "    # placeholders\n",
    "    #data = tf.placeholder(tf.float32, [None, length, num_data_cols])\n",
    "    #target = tf.placeholder(tf.float32, [None])\n",
    "    #dropout_prob = tf.placeholder(tf.float32)\n",
    "    #reg = tf.placeholder(tf.float32)\n",
    "    # 04232022\n",
    "    data = tf.compat.v1.placeholder(tf.float32, [None, length, num_data_cols])\n",
    "    target = tf.compat.v1.placeholder(tf.float32, [None])\n",
    "    dropout_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "    reg = tf.compat.v1.placeholder(tf.float32)   \n",
    "    \n",
    "\n",
    "    # initialization\n",
    "    model = VariableSequenceLabelling(data, target, dropout_prob, reg, num_hidden=NUM_HIDDEN, class_weights=class_weight)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    # 04232022\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    dp = KEEP_PROB\n",
    "    rp = REGULARIZATION\n",
    "    train_samples = x_train.shape[0]\n",
    "    indices = list(range(train_samples))\n",
    "    num_classes = NUM_CLASSES\n",
    "    \n",
    "    # for storing results\n",
    "    test_data = x_test\n",
    "    val_data = x_val\n",
    "\n",
    "    val_aucs = []\n",
    "    test_aucs = []\n",
    "    val_aucs_macro = []\n",
    "    test_aucs_macro = []\n",
    "    test_accuracys = []\n",
    "    test_f1s = []\n",
    "    test_auprcs = []\n",
    "    \n",
    "    epoch = -1\n",
    "\n",
    "    while (epoch < 3 or max(np.diff(early_stop[-3:])) > 0):\n",
    "        epoch += 1\n",
    "        np.random.seed(RANDOM)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        num_batches = train_samples//batch_size\n",
    "        for batch_index in range(num_batches):\n",
    "\n",
    "            sample_indices = indices[batch_index*batch_size:batch_index*batch_size+batch_size]\n",
    "            batch_data = x_train[sample_indices, :, :num_data_cols]\n",
    "            batch_target = y_train[sample_indices]\n",
    "            _, loss = sess.run([model.optimize, model.cross_ent], {data: batch_data, target: batch_target, dropout_prob: dp, reg: rp})\n",
    "\n",
    "            # write train accuracy to log files every 10 batches\n",
    "            #if batch_index % 2000 == 0:\n",
    "            #    loss, prediction, error = sess.run([model.cross_ent, model.prediction, model.error], {data: batch_data, target: batch_target, dropout_prob: dp, reg: rp})\n",
    "            #    #train_writer.add_summary(summaries, global_step=epoch*batch_index)\n",
    "            #    print('Epoch {:2d} Batch {:2d}'.format(epoch+1, batch_index))\n",
    "            #    print('Loss = ', np.mean(loss))\n",
    "            #    print('Error = ', error)\n",
    "\n",
    "        cur_val_preds = sess.run(model.prediction, {data: x_val, target: y_val, dropout_prob: 1, reg: rp}) \n",
    "        val_preds = cur_val_preds[1]\n",
    "        \n",
    "        cur_test_preds = sess.run(model.prediction, {data: x_test, target: y_test, dropout_prob: 1, reg: rp}) \n",
    "        test_preds = cur_test_preds[1]\n",
    "        \n",
    "        val_auc_macro = roc_auc_score(y_val_classes, val_preds, average='macro')\n",
    "        test_auc_macro = roc_auc_score(y_test_classes, test_preds, average='macro')\n",
    "        val_aucs_macro.append(val_auc_macro)\n",
    "        test_aucs_macro.append(test_auc_macro)\n",
    "\n",
    "        val_auc = roc_auc_score(y_val_classes, val_preds, average=None)\n",
    "        test_auc = roc_auc_score(y_test_classes, test_preds, average=None)\n",
    "        val_aucs.append(val_auc)\n",
    "        test_aucs.append(test_auc)\n",
    "        \n",
    "        \n",
    "        idx = np.argmax(cur_test_preds, axis=-1)\n",
    "        test_preds_label = np.zeros(test_preds.shape)\n",
    "        test_preds_label[np.arange(test_preds_label.shape[0]), idx] = 1\n",
    "        test_accuracy = accuracy_score(y_test_classes, test_preds_label)\n",
    "        test_accuracys.append(test_accuracy)\n",
    "\n",
    "        test_f1 = f1_score(y_test_classes, test_preds_label, average='macro')\n",
    "        test_f1s.append(test_f1)\n",
    "\n",
    "        test_auprc = average_precision_score(y_test_classes, test_preds_label, average='macro')\n",
    "        test_auprcs.append(test_auprc)\n",
    "\n",
    "        \n",
    "        if isinstance(val_aucs_macro[-1], dict):\n",
    "            early_stop = [val_auc_macro for val_auc_macro in val_aucs_macro]\n",
    "        else: \n",
    "            early_stop = val_aucs_macro\n",
    "\n",
    "\n",
    "    if isinstance(val_aucs_macro[-1], dict):\n",
    "        best_epoch = np.argmax(np.array([val_auc_macro for val_auc_macro in val_aucs_macro]))\n",
    "    else: \n",
    "        best_epoch = np.argmax(val_aucs_macro)\n",
    "\n",
    "    best_val_auc = val_aucs[best_epoch]\n",
    "    best_test_auc = test_aucs[best_epoch]\n",
    "    best_test_auc_macro = test_aucs_macro[best_epoch]\n",
    "    best_test_accuracy = test_accuracys[best_epoch]\n",
    "    best_test_f1 = test_f1s[best_epoch]\n",
    "    best_test_auprc = test_auprcs[best_epoch]\n",
    "    \n",
    "    print(\"AUC:\")\n",
    "    print(best_test_auc)\n",
    "    print(\"AUC Macro:\")\n",
    "    print(best_test_auc_macro)\n",
    "    print(\"Accuracy: \")\n",
    "    print(best_test_accuracy)\n",
    "    print(\"F1 Macro:\")\n",
    "    print(best_test_f1)\n",
    "    print(\"AUPRC Macro: \")\n",
    "    print(best_test_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
